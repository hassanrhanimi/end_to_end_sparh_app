{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964a7713",
   "metadata": {},
   "source": [
    "# ETL and Machine Learning\n",
    "\n",
    "In this lab I’ll create an Apache Spark Machine learning application as end to end use case from data acquisition, transformation, model training and deployment.\n",
    "\n",
    "Objectives\n",
    "After completing reading, you will see and hopefelly be able also to:\n",
    "\n",
    "- Pull-in data from the HMP dataset <a href=\"https://github.com/wchill/HMP_Dataset\">here</a>\n",
    "- Create a Spark data frame from the raw data\n",
    "- Store this to parquet (in Cloud Object Store)\n",
    "- Read it again (from Cloud Object Store)\n",
    "- Deploy this model to Train a ML-Model on that data set\n",
    "- Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63f740",
   "metadata": {},
   "source": [
    "## 1. Pull-in data from the HMP dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72090e6d",
   "metadata": {},
   "source": [
    "Now it’s time to explore data <a href=\"https://github.com/wchill/HMP_Dataset\">here</a>. You're invited to get familiarize a little bit with it. It's important to understand data so that you can grasp thefollowing step code easily.\n",
    "Let's pull the data in raw format from the source (github)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22bb3284",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 14 kB/s  eta 0:00:01    |███                             | 26.2 MB 4.6 MB/s eta 0:00:56     |███████▎                        | 64.2 MB 6.0 MB/s eta 0:00:37     |█████████████████▏              | 151.0 MB 7.3 MB/s eta 0:00:18     |█████████████████████▍          | 188.2 MB 5.7 MB/s eta 0:00:17     |██████████████████████████▋     | 233.8 MB 7.1 MB/s eta 0:00:07     |███████████████████████████████▎| 274.5 MB 6.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=ff6aa6dfb729a8007bfc4d584b0bd125dccee78b79820147a37fdfc3262d0441\n",
      "  Stored in directory: /home/mbg/.cache/pip/wheels/23/f6/d3/110e53bd43baeb8d7d38049733d48e39cbecd056f01dba7ee8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
      "Collecting findspark\n",
      "  Using cached findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"SparkApp_ETL_ML\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_parquet(\"https://s3.eu-de.cloud-object-storage.appdomain.cloud/cloud-object-storage-yy-cos-standard-js4/data.parquet\")\n",
    "\n",
    "sdf = spark.createDataFrame(df)\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "sdf = sdf.withColumn(\"x\", sdf.x.cast(DoubleType()))\n",
    "sdf = sdf.withColumn(\"y\", sdf.y.cast(DoubleType()))\n",
    "sdf = sdf.withColumn(\"z\", sdf.z.cast(DoubleType()))\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "input_columns = [\"x\", \"y\", \"z\"]  # input columns to consider\n",
    "train, test = sdf.randomSplit([0.8, 0.2], seed=1)\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "vectorAssembler = VectorAssembler(inputCols=input_columns, outputCol=\"features\")\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")\n",
    "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer])\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\"). \\\n",
    "    setLabelCol(\"label\")\n",
    "df_train = pipeline.fit(train).transform(train)\n",
    "df_test = pipeline.fit(test).transform(test)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features_norm', labelCol='label', maxDepth=20, numTrees=7, seed=1)\n",
    "rfModel = rf.fit(df_train)\n",
    "\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "model_target = \"HMP_frModel.xml\" \n",
    "\n",
    "pmmlBuilder = PMMLBuilder(sc, df_train, rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b92f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark2pmml==0.5.1 in /home/mbg/.local/lib/python3.8/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: py4j in /home/mbg/.local/lib/python3.8/site-packages (from pyspark2pmml==0.5.1) (0.10.9.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark2pmml==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf734e04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark2pmml import PMMLBuilder\n",
    "model_target = \"HMP_frModel.xml\"       # model output file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "475edbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkApp_ETL_ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb7fc0b7c70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06092f0f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/mbg/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0/wget-3.2-py3-none-any.whl\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4bce8e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import site\n",
    "# import wget\n",
    "# url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "#            'jpmml-sparkml-executable-1.7.2.jar')\n",
    "# wget.download(url)\n",
    "# # shutil.copy('jpmml-sparkml-executable-1.7.2.jar', site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "# shutil.copy('jpmml-sparkml-executable-1.7.2.jar', '~/.local/lib/python3.8/site-packages/pyspark/jars')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd382d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Thread-4\" java.lang.ExceptionInInitializerError\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat py4j.reflection.CurrentThreadClassLoadingStrategy.classForName(CurrentThreadClassLoadingStrategy.java:40)\n",
      "\tat py4j.reflection.ReflectionUtil.classForName(ReflectionUtil.java:51)\n",
      "\tat py4j.reflection.TypeUtil.forName(TypeUtil.java:243)\n",
      "\tat py4j.commands.ReflectionCommand.getUnknownMember(ReflectionCommand.java:175)\n",
      "\tat py4j.commands.ReflectionCommand.execute(ReflectionCommand.java:87)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.IllegalArgumentException: Expected Apache Spark ML version 3.1, got version 3.2 (3.2.0)\n",
      "\tat org.jpmml.sparkml.ConverterFactory.checkVersion(ConverterFactory.java:114)\n",
      "\tat org.jpmml.sparkml.PMMLBuilder.init(PMMLBuilder.java:481)\n",
      "\tat org.jpmml.sparkml.PMMLBuilder.<clinit>(PMMLBuilder.java:545)\n",
      "\t... 10 more\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mbg/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mbg/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/mbg/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org.jpmml.sparkml.PMMLBuilder does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20251/3496938591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpmmlBuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPMMLBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark2pmml/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sc, df, pipelineModel)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mjavaSchema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjavaDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mjavaPipelineModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mjavaPmmlBuilderClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjpmml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPMMLBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjavaPmmlBuilderClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JPMML-SparkML not found on classpath\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1647\u001b[0m                 answer[proto.CLASS_FQN_START:], self._gateway_client)\n\u001b[1;32m   1648\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1649\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} does not exist in the JVM\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_fqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: org.jpmml.sparkml.PMMLBuilder does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "pmmlBuilder = PMMLBuilder(sc, df_train, rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmmlBuilder.buildFile(model_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
